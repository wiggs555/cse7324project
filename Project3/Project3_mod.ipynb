{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE 7324 Lab 3: Extending Logistic Regression**\n",
    "### *Thomas Adams, Suleiman Hijazeen, Nancy Le and Andrew Whigham*\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preparation and Overview**\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies for lab 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "# use plotly in offline mode to not have active connection to plotly servers\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import warnings\n",
    "#warnings.simplefilter('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breed</th>\n",
       "      <th>color</th>\n",
       "      <th>count</th>\n",
       "      <th>sex</th>\n",
       "      <th>Spay/Neuter</th>\n",
       "      <th>Periods</th>\n",
       "      <th>Period Range</th>\n",
       "      <th>outcome_age_(days)</th>\n",
       "      <th>outcome_age_(years)</th>\n",
       "      <th>Cat/Kitten (outcome)</th>\n",
       "      <th>...</th>\n",
       "      <th>dob_day</th>\n",
       "      <th>dob_dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>quarter</th>\n",
       "      <th>spayed_neutered</th>\n",
       "      <th>domestic</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>domestic shorthair</td>\n",
       "      <td>orange</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.038356</td>\n",
       "      <td>Kitten</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>transfer_partner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>domestic shorthair</td>\n",
       "      <td>blue</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>Kitten</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>adoption_unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>domestic shorthair</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>90</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>Kitten</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>adoption_offsite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domestic mediumhair</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Cat</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>return-to-owner_unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>domestic shorthair</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>0.057534</td>\n",
       "      <td>Kitten</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>transfer_partner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 breed   color  count     sex Spay/Neuter  Periods  \\\n",
       "0   domestic shorthair  orange      1    male          No        2   \n",
       "1   domestic shorthair   blue       1  female          No        1   \n",
       "2   domestic shorthair   white      1  female         Yes        3   \n",
       "3  domestic mediumhair   black      1  female         Yes        1   \n",
       "4   domestic shorthair   black      1    male          No        3   \n",
       "\n",
       "   Period Range  outcome_age_(days)  outcome_age_(years) Cat/Kitten (outcome)  \\\n",
       "0             7                  14             0.038356               Kitten   \n",
       "1            30                  30             0.082192               Kitten   \n",
       "2            30                  90             0.246575               Kitten   \n",
       "3           365                 365             1.000000                  Cat   \n",
       "4             7                  21             0.057534               Kitten   \n",
       "\n",
       "            ...            dob_day dob_dayofweek  month  day dayofweek  hour  \\\n",
       "0           ...                  7             0      7   22         1    16   \n",
       "1           ...                 16             0      8   14         3    18   \n",
       "2           ...                 26             2      6   29         6    17   \n",
       "3           ...                 27             2      3   28         4    14   \n",
       "4           ...                 16             0      1    9         3    19   \n",
       "\n",
       "   quarter spayed_neutered  domestic                  outcome  \n",
       "0        3           False         1         transfer_partner  \n",
       "1        3           False         1         adoption_unknown  \n",
       "2        2            True         1         adoption_offsite  \n",
       "3        1            True         1  return-to-owner_unknown  \n",
       "4        1           False         1         transfer_partner  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "shelter_outcomes = pd.read_csv(\"C:/Users/sulem/OneDrive/Desktop/machin learnign/Project3/aac_shelter_cat_outcome_eng.csv\")\n",
    "# filter animal type for just cats\n",
    "cats = shelter_outcomes[shelter_outcomes['animal_type'] == 'Cat']\n",
    "#print(cats.head())\n",
    "\n",
    "# remove age_upon_outcome and recalculate to standard units (days)\n",
    "age = cats.loc[:,['datetime', 'date_of_birth']]\n",
    "# convert to datetime\n",
    "age.loc[:,'datetime'] = pd.to_datetime(age['datetime'])\n",
    "age.loc[:,'date_of_birth'] = pd.to_datetime(age['date_of_birth'])\n",
    "# calculate cat age in days\n",
    "cats.loc[:,'age'] = (age.loc[:,'datetime'] - age.loc[:,'date_of_birth']).dt.days\n",
    "# get dob info\n",
    "cats['dob_month'] = age.loc[:, 'date_of_birth'].dt.month\n",
    "cats['dob_day'] = age.loc[:, 'date_of_birth'].dt.day\n",
    "cats['dob_dayofweek'] = age.loc[:, 'date_of_birth'].dt.dayofweek\n",
    "# get month from datetime\n",
    "cats['month'] = age.loc[:,'datetime'].dt.month\n",
    "# get day of month\n",
    "cats['day'] = age.loc[:,'datetime'].dt.day\n",
    "# get day of week\n",
    "cats['dayofweek'] = age.loc[:, 'datetime'].dt.dayofweek\n",
    "# get hour of day\n",
    "cats['hour'] = age.loc[:, 'datetime'].dt.hour\n",
    "# get quarter\n",
    "cats['quarter'] = age.loc[:, 'datetime'].dt.quarter\n",
    "\n",
    "# clean up breed attribute\n",
    "# get breed attribute for processing\n",
    "# convert to lowercase, remove mix and strip whitespace\n",
    "# remove space in 'medium hair' to match 'longhair' and 'shorthair'\n",
    "# split on either space or '/'\n",
    "breed = cats.loc[:, 'breed'].str.lower().str.replace('mix', '').str.replace('medium hair', 'mediumhair').str.strip().str.split('/', expand=True)\n",
    "cats['breed'] = breed[0]\n",
    "cats['breed1'] = breed[1]\n",
    "\n",
    "# clean up color attribute\n",
    "# convert to lowercase\n",
    "# strip spaces\n",
    "# split on '/'\n",
    "color = cats.loc[:, 'color'].str.lower().str.strip().str.split('/', expand=True)\n",
    "cats['color'] = color[0]\n",
    "cats['color1'] = color[1]\n",
    "\n",
    "# clean up sex_upon_outcome\n",
    "sex = cats['sex_upon_outcome'].str.lower().str.strip().str.split(' ', expand=True)\n",
    "sex[0].replace('spayed', True, inplace=True)\n",
    "sex[0].replace('neutered', True, inplace=True)\n",
    "sex[0].replace('intact', False, inplace=True)\n",
    "sex[1].replace(np.nan, 'unknown', inplace=True)\n",
    "cats['spayed_neutered'] = sex[0]\n",
    "cats['sex'] = sex[1]\n",
    "\n",
    "# add in domesticated attribute\n",
    "cats['domestic'] = np.where(cats['breed'].str.contains('domestic'), 1, 0)\n",
    "\n",
    "# combine outcome and outcome subtype into a single attribute\n",
    "cats['outcome_subtype'] = cats['outcome_subtype'].str.lower().str.replace(' ', '-').fillna('unknown')\n",
    "cats['outcome_type'] = cats['outcome_type'].str.lower().str.replace(' ', '-').fillna('unknown')\n",
    "cats['outcome'] = cats['outcome_type'] + '_' + cats['outcome_subtype']\n",
    "\n",
    "# drop unnecessary columns\n",
    "cats.drop(columns=['animal_id', 'name', 'animal_type', 'age_upon_outcome', 'date_of_birth', 'datetime', 'monthyear', 'sex_upon_outcome', 'outcome_subtype', 'outcome_type'], inplace=True)\n",
    "#print(cats['outcome'].value_counts())\n",
    "\n",
    "cats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.drop(columns=['breed1'], inplace=True)\n",
    "cats.drop(columns=['color2'], inplace=True)\n",
    "cats.drop(columns=['breed2'], inplace=True)\n",
    "cats.drop(columns=['coat_pattern'], inplace=True)\n",
    "cats.drop(columns=['Cat/Kitten (outcome)'], inplace=True)\n",
    "cats.drop(columns=['sex_age_outcome'], inplace=True)\n",
    "cats.drop(columns=['dob_monthyear'], inplace=True)\n",
    "cats.drop(columns=['outcome_weekday'], inplace=True)\n",
    "#Breed, Color, Color1, Spayed_Netured and Sex attributes need to be one hot encoded\n",
    "cats_ohe = pd.get_dummies(cats, columns=['breed', 'color', 'color1', 'spayed_neutered', 'sex','Spay/Neuter','age_group','coat','domestic_breed','cfa_breed' ])\n",
    "#out_t={'euthanasia_rabies-risk' : 1, 'unknown_unknown' : 2, 'adoption_barn' : 3, 'died_unknown' : 4, 'adoption_offsite' : 5, 'adoption_unknown' : 6, 'missing_in-foster' : 7, 'rto-adopt_unknown' : 8, 'died_enroute' : 9, 'died_in-surgery' : 10, 'transfer_snr' : 11, 'euthanasia_medical' : 12, 'euthanasia_aggressive' : 13, 'transfer_scrp' : 15, 'euthanasia_unknown' : 14, 'missing_unknown' : 16, 'died_in-foster' : 17, 'missing_possible-theft' : 18, 'adoption_foster' : 19, 'euthanasia_at-vet' : 20, 'missing_in-kennel' : 21, 'died_at-vet' : 22, 'transfer_partner' : 23, 'return-to-owner_unknown' : 25, 'disposal_unknown' : 24, 'euthanasia_underage' : 26, 'died_in-kennel' : 27, 'euthanasia_suffering' : 28, 'transfer_barn' : 29}\n",
    "#out_t={'euthanasia_rabies-risk' : 1, 'unknown_unknown' : 2, 'adoption_barn' : 3, 'died_unknown' : 4, 'adoption_offsite' : 5, 'adoption_unknown' : 6, 'missing_in-foster' : 7, 'rto-adopt_unknown' : 8, 'died_enroute' : 9, 'died_in-surgery' : 10, 'transfer_snr' : 11, 'euthanasia_medical' : 12, 'euthanasia_aggressive' : 13, 'transfer_scrp' : 15, 'euthanasia_unknown' : 14, 'missing_unknown' : 16, 'died_in-foster' : 0, 'missing_possible-theft' : 0, 'adoption_foster' : 0, 'euthanasia_at-vet' : 0, 'missing_in-kennel' : 0, 'died_at-vet' : 0, 'transfer_partner' : 0, 'return-to-owner_unknown' : 0, 'disposal_unknown' : 0, 'euthanasia_underage' : 0, 'died_in-kennel' : 0, 'euthanasia_suffering' : 0, 'transfer_barn' : 0}\n",
    "out_t={'euthanasia_suffering' : 0, 'died_in-kennel' : 0, 'return-to-owner_unknown' : 0, 'transfer_partner' : 1, 'euthanasia_at-vet' : 2, 'adoption_foster' : 3, 'died_in-foster' : 0, 'transfer_scrp' : 4, 'euthanasia_medical' : 0, 'transfer_snr' : 0, 'died_enroute' : 0, 'rto-adopt_unknown' : 0, 'missing_in-foster' : 0, 'adoption_offsite' : 0, 'adoption_unknown' :5,'euthanasia_rabies-risk' : 0, 'unknown_unknown' : 0, 'adoption_barn' : 0, 'died_unknown' : 0, 'died_in-surgery' : 0, 'euthanasia_aggressive' : 0, 'euthanasia_unknown' : 0, 'missing_unknown' : 0, 'missing_in-kennel' : 0, 'missing_possible-theft' : 0, 'died_at-vet' : 0, 'disposal_unknown' : 0, 'euthanasia_underage' : 0, 'transfer_barn' : 0}\n",
    "\n",
    "cats_ohe.head()\n",
    "\n",
    "# separate outcome from data\n",
    "outcome = cats_ohe['outcome']\n",
    "cats_ohe.drop(columns=['outcome'])\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(cats_ohe, outcome, test_size=0.2, random_state=0)\n",
    "X_train.drop(columns=['outcome'], inplace=True)\n",
    "y_train = [out_t[item] for item in y_train]\n",
    "#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations, C):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C=C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    # inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "    \n",
    "    \n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(eta,X,y,w,grad,C=0.001):\n",
    "        wnew = w - grad*eta\n",
    "        g = expit(X @ wnew)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb,y)\n",
    "            # minimization inopposite direction\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/50} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.objective_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,0.001), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ -= gradient*eta # set new function values\n",
    "            # subtract to minimize\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))    \n",
    "        \n",
    "        \n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained MultiClass Logistic Regression Object\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations,solver='leaner', C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.slv  = solver\n",
    "        self.C=C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            if self.slv=='stochastic':\n",
    "             slr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "             slr.fit(X,y_binary)\n",
    "             self.classifiers_.append(slr)\n",
    "            if self.slv=='steepest':\n",
    "             mls=LineSearchLogisticRegression(self.eta,self.iters,self.C)\n",
    "             mls.fit(X,y_binary)\n",
    "             self.classifiers_.append(mls)\n",
    "            if self.slv=='leaner':\n",
    "             blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "             blr.fit(X,y_binary)\n",
    "             self.classifiers_.append(blr)\n",
    "            if self.slv=='newton':\n",
    "             bfgslr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "             bfgslr.fit(X,y_binary)\n",
    "             self.classifiers_.append(bfgslr)\n",
    "            if self.slv=='newton1':\n",
    "             newt = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "             newt.fit(X,y_binary)\n",
    "             self.classifiers_.append(newt)\n",
    "            \n",
    "            # add the trained classifier to the list      \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row   \n",
    "lr = LogisticRegression(0.1,500)\n",
    "print(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23536, 186)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-3.42179002e-07 -3.42179002e-07 -9.74719719e-07 ... -3.24873073e-07\n",
      "  -3.24450978e-07 -1.77280239e-08]\n",
      " [-1.60273891e-07 -1.60273891e-07 -5.96495250e-07 ... -1.48531859e-07\n",
      "  -1.48124284e-07 -1.21496072e-08]\n",
      " [-4.28419043e-05 -4.28419043e-05 -1.37021925e-04 ... -4.03724989e-05\n",
      "  -4.03196482e-05 -2.52225606e-06]\n",
      " [-6.64750903e-07 -6.64750903e-07 -2.14794347e-06 ... -6.28685097e-07\n",
      "  -6.27844658e-07 -3.69062451e-08]\n",
      " [-1.01475511e-06 -1.01475491e-06 -5.73583629e-05 ...  3.33525122e-06\n",
      "   3.26613571e-06 -4.28089062e-06]\n",
      " [-1.66100620e-07 -1.66100620e-07 -5.24855231e-07 ... -1.57596723e-07\n",
      "  -1.57577783e-07 -8.52283682e-09]]\n",
      "Accuracy of:  0.34721278042148196\n",
      "Wall time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "x_train_ar=X_train.values\n",
    "y_target_ar=np.asarray(y_train)\n",
    "lr = LogisticRegression(.01,10,'newton',.0001)\n",
    "lr.fit(x_train_ar,y_target_ar)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(x_train_ar)\n",
    "print('Accuracy of: ',accuracy_score(y_target_ar,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.67380691e-07 -3.66794290e-07  3.83305102e-05 ... -4.08537452e-06\n",
      "  -3.83871477e-06  3.47192048e-06]\n",
      " [ 7.51654247e-06  6.93515648e-06 -2.85991340e-02 ...  4.88418458e-04\n",
      "   5.51419366e-04 -5.44484210e-04]\n",
      " [-9.41664284e-07 -9.40846782e-07 -2.96156727e-06 ... -8.15397438e-07\n",
      "  -8.12750966e-07 -1.28095816e-07]\n",
      " [-3.58179906e-07 -3.56479505e-07 -2.31702188e-04 ... -2.73421503e-05\n",
      "  -2.72927240e-05  2.69362445e-05]\n",
      " [-3.21584353e-07 -3.21465419e-07  2.02699941e-05 ...  1.96504127e-06\n",
      "   1.92922288e-06 -2.25068830e-06]\n",
      " [-2.64656952e-06 -2.59923671e-06  4.50203618e-04 ... -1.35685183e-04\n",
      "  -1.56286055e-04  1.53686818e-04]]\n",
      "Accuracy of:  0.4839394969408566\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='newton-cg',n_jobs=2,C=.0001, max_iter=10) \n",
    "x_train_ar=X_train.values\n",
    "y_target_ar=np.asarray(y_train)\n",
    "lr_sk.fit(x_train_ar,y_target_ar)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(x_train_ar)\n",
    "print('Accuracy of: ',accuracy_score(y_target_ar,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Modeling**\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Deployment**\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Optimization Using Mean Squared Error**\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. References**\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
