{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE 7324 Lab 6: Convolutional Network Architectures**\n",
    "### *Thomas Adams, Suleiman Hijazeen, Nancy Le and Andrew Whigham*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preparation**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from scipy import ndimage\n",
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "from time import sleep\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "from plotly.graph_objs import Bar, Line\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "from plotly.graph_objs.scatter import Marker\n",
    "from plotly.graph_objs.layout import XAxis, YAxis\n",
    "import seaborn as sns\n",
    "from IPython.display import Image as _Imgdis\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "import cv2  \n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "\n",
    "keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.io import imshow\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def summarize_net(net, X_test, y_test, title_text=''):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    yhat = np.argmax(net.predict(X_test), axis=1)\n",
    "    acc = mt.accuracy_score(y_test,yhat)\n",
    "    cm = mt.confusion_matrix(y_test,yhat)\n",
    "    cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f')\n",
    "    plt.title(title_text+'{:.4f}'.format(acc))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Metric Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom f1 metric from custom recall and precision\n",
    "from keras.layers import concatenate\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred): # recall is true positive / (total actual positive)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # y_true * y_pred will only give 1 for true positives\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) # actual positives are just y_true\n",
    "        # recall is true positive / (total actual positive).. the episol is a small number to prevent divide by zero errors\n",
    "        recall = true_positives / (possible_positives + K.epsilon()) \n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred): #precision is true positives / (total predicted positives)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # y_true * y_pred will only give 1 for true positives\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) # predicted positives\n",
    "        # (true positive / predicted positive).. the episol is a small number to prevent divide by zero errors\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())   \n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred): # f1 = 2 * (precision*recall / precision + recall)\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric we will use to determine the performance of our model will be the macro-averaged F1 score. We are using macro-averaging instead of micro even though we do not have a class imbalance problem. The number of instances for each class are 3000 \n",
    "\n",
    " in this project we are predecting what latter this sign is for using a set of pictures of ASL signes thus We are using F1 as we care both about precision and recall , either high False Positive or high False Negative leades to missunderstading of a latter that lead to a missunderstading in the whold word or sentenanse.\n",
    "\n",
    "Since keras does not provide recall, precision, or f1 in their metrics package as a result of the 2.0 release, we will need to implement our own custom metric. Keras removed these functions as they are global metrics which were being approximated in batches (as keras runs in batches). However, for our purposes, this approximation will suffice. We found the following post on datascience stackexchange which helped detail this process below. Though fairly straightforward, we have provided comments to explain the code we have leveraged.\n",
    "\n",
    "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "\n",
    "https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "asl = pd.read_csv(\"data/asl_alphabet_train_50.csv\")\n",
    "\n",
    "y=asl.drop(asl.columns[1:], axis=1)\n",
    "asl=asl.drop(asl.columns[0], axis=1)\n",
    "asl.shape\n",
    "yasl=np.asarray(asl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(yasl.reshape((-1,50,50)), axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Splitting Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_wh=50\n",
    "NUM_CLASSES=29\n",
    "X_ar=np.asarray(asl)\n",
    "y_ar=np.asarray(y)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_ar,y_ar, test_size=0.2)\n",
    "X_train_d = np.expand_dims(X_train.reshape((-1,img_wh,img_wh)), axis=3)\n",
    "X_test_d = np.expand_dims(X_test.reshape((-1,img_wh,img_wh)), axis=3)\n",
    "\n",
    "y_train.shape\n",
    "X_train.shape\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Data\n",
    "Here we will split the training and test datasets. Since we have almost 80k instances of data, the likelihood that we will use ~64k unrepresentative examples is extremely small. Thus, we have opted for the simple 80/20 split.\n",
    "\n",
    "since we got 3000 train picture for each alphabet so I believe this is enough figure to go with a simple 80 20 split  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Modeling**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 CNN with Keras and Data Expansion\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=10, # used, Int. Degree range for random rotations.\n",
    "    width_shift_range=0.1, # used, Float (fraction of total width). Range for random horizontal shifts.\n",
    "    height_shift_range=0.1, # used,  Float (fraction of total height). Range for random vertical shifts.\n",
    "    shear_range=1, # Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "    zoom_range=0.1,#Range for random zoom\n",
    "    channel_shift_range=0.1,#Range for random channel shifts.\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rescale=None)\n",
    "\n",
    "datagen.fit(X_train_d)\n",
    "\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code above will alter our training data so that for each epoces you are always dealing with new data, we decide to generate different pictures by changing the following:\n",
    "\n",
    "1- rotation_range: which is routing the picture by a certain value of degrees, we decicde to be 10 because more than that might introduce confusion with other ASL signs\n",
    "\n",
    "2- height_shift_range: inducing a random horizontal shifts we decided to stay with .1 (mean shift .1 fraction of the total height ) because we don't to loss any features in the picture by adding high shifts \n",
    "\n",
    "3- width_shift_range: introducing a random vertical shifts we decided to stay with .1 (mean shift .1 fraction of the total width) because we don't lose any features in the picture by adding high shifts \n",
    "\n",
    "4- shear_range: adding a sheer intensity, we believe adding shear will give better mimicry to real cases of destortion to an image \n",
    "\n",
    "5- zoom_range: introducing random zoom which is a good representation to a distortion might happen to a picture where not the whole hand is visible \n",
    "\n",
    "6- channel_shift_range: random channel shifts, this might not affect the picture but we thought its good element to add if colors would be introduced in the future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXusXNd13r91Z+bOffBx+RYtUpZsK7YV6+VQihApkaDYgGKnkZsqrh0jUAABQpIWldGksdyghYMWiFwXsf9o4VaxHAtoYDm2E0hR/IhMS5XVphRpkbYetERaokRaFCk+L3lf89r94w6p2d9ad/a+w8u5lznrBxDkPrP3OXvOOZtn1nfWQ0IIcBynWAws9gQcx+k/vvAdp4D4wnecAuIL33EKiC98xykgvvAdp4D4wnecAuIL33EKyDktfBG5TUReFJG9InLvQk3KcZzzi/TquSciJQAvAfgggAMAtgP4eAjhhbnGVCqDoVodOdsulSq6E82HZ1cul9WQgVL8/9fAgP7/jL8n7ze0WmqMGPuhnXb/3OiSdb6z9hu6ts1t1G71dO0zjpMxZimTc410H26LNSq5X5EBakvXz5np6QnU6zPWwSP0KsrnegB7Qwgvtyf4EIDbAcy58KvVEVz5vl852145tl714QXYCs2oPTa2To0ZHVsWtUeWD6s+tZl6fBy6cDOTM3q+I1W1LZpbQ/9nwTTqjbhda6g+PJdmM/7OaOkbptGI+zQbddWnVp+Oj0PntlbX3zlFs6mP02o1qR0fJ+8/u/S5BN/01pjEwsg5TqOprxHD57vZisfwggXyzsPgYHzvlsuD1DYelh3s2rU1eQzg3H7qXwxgf0f7QHub4zhLnHN54mf9lhGRuwHcDej/zRzHWRzOZeEfALC5o70JwOvcKYRwP4D7AWDtureFd1/5/rOfXfnL71M7nRifjNr8E3zyVPx5+yBRs2X8NK4MxT+ZWs34J9/I8hEw/FN/bN1Y3F6/Uo0ZWTkatcvlkp4vMxD/H8rfuUpzB4AmzZ9NCkCbFXUyd+o1/bOdjz11aipqT0/E5gMATJw4HbXZrOLjAkDTmC8zScduGOYM02jUojbrPfV6/DmALE1FYfyU7zYPQJsHLcPsYLOJx9Rq8TmZHdPq+HdTfW5xLj/1twO4XEQuE5FBAB8D8Mg57M9xnD7R8xM/hNAQkX8N4LsASgC+HEJ4fsFm5jjOeeNcfuojhPAtAN9aoLk4jtMn3HPPcQrIOT3x532wShlrN6092775pverPnX1rpkFES3ETNViIWViRr+fnjjdXaRqGELX1Om4T30mPs6JwyfVmEOvHo7H0H6t9/gsfrHANmz4JSwj34WxdVpoXLVxddRevWFVvN9BQzSk88/tOvsYAKhPx+eFfQzq09a5ja9HbVqLYXweJk6e7vo5oAVB7mMdh4XRZp3mX9PH4ff2/I6+1dLXWSrx+c55rz8wHD+bZ2a0uFetvnV/mE5x1n6zejmO808KX/iOU0B84TtOAemrjQ+RyKGiYgTcCNmQw5XYZmkYNuaK4dgGNm0ncvFnrWDAcMhgvYF90K251Ggb6w+1hrb9pqZjG5L1B9OWJUen8WOnVJ/xo+Pxfqbi/TSNWIMm2eesSQwOa12gOhw7Oq0kvWH9JTomY/3GNVHbOi8MXzPLAYnny45aljMRO4VNjcd29LE3jqkxfC75GllBX6wZWU5X05PstBTfP5XKkBrT6bRjxQhY+BPfcQqIL3zHKSC+8B2ngPTVxhcAA6W3bJCykeiCbZR6Ix3nXKL9WLZ3aoylC1Soj5TigJtmSQfgjNL8Vg7PPyLR0hsY1h8sUj4QJ6f0O2HWJNh2PX1cawkcWMU+EXt37lVjWEuwzj/b6zwXlbcAwOiKOEhq1YY4sGqdoTe87Z1vi9pDlfS78Cny6eD5W1oCazcT4xOqz8SJeNvp47HvghUk1emb8MOd3XNInMGf+I5TQHzhO04B8YXvOAXEF77jFJA+O/AgmbmEHXYGDQGNYdEqR5yZyXAYYSGRHXjKxtxSgReWOJkjRjIsPFrBS1YQTidDxuclml+Ts/lekpMptvt5A4BpEh7NwKqZ7o5NnPkH0E4yLKDte3afGmMJcd32AehAHr7ug0bWpHWbYy+ylWt1YNXm92yO2uwc1TDu2865fOdbf6k+t/AnvuMUEF/4jlNAfOE7TgHpr41PsBMNoBM/5IxBhjMLUyH7PCcpAlfxscbw/Nj2Nh2FaC6NjIIUOfNl7YA1iRy9YaCHDLTsXGRVNmL9wdIjxkYo83GcRwTh4oxzkEgsAujvzPrP5LTWHziRCAdNmdmIT8ZawZGfHVF9Dr5yMGpz0RYrMKkzS/CUlYXawJ/4jlNAfOE7TgHxhe84BcQXvuMUkP6KewG9lSvqICX+zUUq4461VxaGMophLcxxaEwpw4mJM/8AwDBHE9Lc+DiAFvysPgzPv0zHsbLrWIIfw85bqYxIgL4/2JnLun9KVXKSoXPJGZ4AAKviqD/eb45wbc2FsxhzH3Z8AuIsQw9/Y1R9buFPfMcpIL7wHaeA+MJ3nAKyqEE6VmCJsokzsuHmoBxeaD/WfjlgKCcTLMO2t5V16N0XXRT3Idv2xYOxUwegv49li6dsesuBp0zbchyFUs5Qlj2fCgay9sPaQY7ew7a2dQ352BU+TxlBVDlaAp8nqyoR7ye1HoD4Ow4amast/InvOAXEF77jFBBf+I5TQBY1SMeyq1WQC9lBObqARSp4xoTstJwqJWxD8jvh7/3jLjXm77/8t1F7dDR+R3zzR29RYz548/VRO+d9O5Njv+fAtqpKSmHYnfzuPwc+/zmJUFhfsJKPcCZn3q+VKIWPw3Ozzi3fc9b8U3OzzmXnd/RKOo7jzIkvfMcpIMmFLyJfFpHDIvJcx7bVIvKYiOxp/72q2z4cx1la5DzxvwLgNtp2L4CtIYTLAWxttx3HuUBIinshhCdF5FLafDuAW9r/fhDAEwA+tRATSjllWKLJS4cORe1Tk7o01PXvemfU5vJSlmMNzyVHwFHOKiQMWVlep6bibLGTk3GZqiMHdKaWHFSwDzuzGKLVVZvjLK9vnDwZtQ+Px6W3gbQDj5mpKMOBh68Jn39LIOQxvF8rsCeVmcgq5873T0rsA/R3tkRpJfiRGGkJjZ3zy3Vw69XG3xBCOAgA7b91QTLHcZYs513cE5G7RWSHiOyYnNB50B3H6T+9LvxDIrIRANp/H56rYwjh/hDClhDClpHRZT0eznGchaRXB55HANwJ4L723w9njaJEHDnOONyuGTba5+75s6g9Oant0NWr40CY++7/TNQuJ6rOAHl2HCca4T77XnhFDWk04pLLpYHYzitX0pfJSvyQClCxdI0/+S9/EbX3PLMnan/qz35fjUmdO2tufO1zkpwsRGASMhKA5DhDDVIwDWsH1r2RU/KdrxE77FjZiDv3s2AOPCLyVQD/CODdInJARO7C7IL/oIjsAfDBdttxnAuEHFX/43N89KsLPBfHcfqEe+45TgFZckE6TE4lGqZe15VPjh17I2ofovfTm9asUWPYBs5KisDfiWy0iYkTakyjTjZ+NU7uODCgz5Oydw3bVSVupDGP/v3/VmOe/Lt/iOfWiN9XP//afjXmF975DrWtkxwtx3qPb1ZNSnxuJbeIxnT9dO65MDk2PWP5A/TSh+klOY0/8R2ngPjCd5wC4gvfcQqIL3zHKSCLKu5ZpDLjWE4nP/e+K6P2th88pvpMT8clir/91a1R+64/uEMfjAM+MgQdtY2zrpQNBwz+TsLttHhjiVopIfTJv31cbWPnpwFyJtq8bq0ak1OhJ0Wu40kKDhhSQV+W008P2YBSlYBynIssVGbhDIejrGxShD/xHaeA+MJ3nALiC99xCsgFV0nHsgVv/ujNUXvX0/9H9ZmmZBfPbtsRtZu/95v2nOcJz18lhmjqaqdMlRx4RlfqCqh8HiwnDrYpDxw9GrWPHn1dj6H5DQ/HEZVW5dicCj1MjmMWW97K6acH2zzlFARoJ59e7lPr+/C9YGowiXNnnVt34HEcJwtf+I5TQHzhO04B6a+NT4k4rGSJg/QeNicw5oq3xwkiy+WK6sO206lTx6I2B+0AOnBnId6pckAOAJRKdBn4vXgpXW3Wgm38V1+Nq+7WajopKduQQ0OxvpCT8KOXwKqc/bJNb53r1Ht8y98hdY9Z1WtUkBR9nqM/5LzX5z7DiSCeXGvfn/iOU0B84TtOAfGF7zgFxBe+4xSQvop7AUCr+ZZYYQlUMxmZSBnOWnLRRTojzP7XdkftwcHYEeUH39mmxnzst38tavciUh09HTsOtYIWfZTgp31kFEoYMsQknu+O72yP2qWSFkErlTiIaNWquFZKKjgFyAiUMbZZ4l4vgTupDDxmye7EGOtznm9O9R3O2mN9Zz4vgwmBEwCGOjL+eplsx3HmxBe+4xQQX/iOU0AWNRFHToVUxrLh2O75xQ/foPoc/svXojZXr9n5RGz/AsA//60PxHOrVrvOzZrLvsNvRu1Wq7s9CWgdYPKUdrTJCYxhG/+FXc/EczUChthh58qbr+p6XEDbs1ZF2hTWdWWtIIeUM45lr6f0BlXBFvrccgUcK3Am5ZBkwdfVmkunI1yODgX4E99xCokvfMcpIL7wHaeA9NnGD5ENYtkjjYSNkmMXXfdLV6ltW7/27ag9OREH5UxM6CCd7T/6SdS+8RfeF7UtG5R1C642Ozl5So1pkK3N1Wus88THCcZ5eWr7j6P21FScSNPSBTjAafN74gAoS5dhmz4nmEa99zbOZeqdfE4CipwgL54L2+I5djMfJyfhh2mv03fmtrXfyLfC3+M7jjMXvvAdp4D4wnecAuIL33EKSJ/FPYmElFRwhEVOYMO65ctVHw4uqRmltJlvPfB3UfvaK38uao8M6qo4PJeTb2rRkClTsAxn5Nnw9g3JfVgC1KNfejhqswhXqaQdksbG4nNpOfCwCNdL1tcc1Hc0jpPKgNSLU5AlgvK9q45riK2WmLcQc4nOizvwOI4zF77wHaeAJBe+iGwWkcdFZLeIPC8i97S3rxaRx0RkT/vvVed/uo7jLAQ5Nn4DwB+GEJ4RkeUAfigijwH4XQBbQwj3ici9AO4F8KmuewohsjPN6im0je1Hy0bLqbAytjpOKDE+HleVmalNqzEnT8YBNi++eiBq//xll6gxTKkcz7c6OKT6NJpxgAdX1L14Uzx3QJ+78SkdyHP8+BtRm+1DK0iHk3O8Y70+dgq+hpbNn7rOFilHm9kdd3ceWqjqOyqzcMrmzySZpdkKZuoMklooB54QwsEQwjPtf58CsBvAxQBuB/Bgu9uDAD6SdUTHcRadedn4InIpgGsBbAOwIYRwEJj9zwHA/B8NjuMsCtkLX0SWAfgmgE+GEMZT/TvG3S0iO0Rkx+Tk6fQAx3HOO1kLX0QqmF30fxVC+Jv25kMisrH9+UYAh62xIYT7QwhbQghbRkaWWV0cx+kzSXFPZhWVBwDsDiH8ecdHjwC4E8B97b8fNobPGxaC2PHGEopY9LFEw1v+5S1R+399Lo6aazYn1JgaCX47vhuX1v7530uLe80GRVcZ5b04484gOdYsH9KC4Ew9FuYe+K9fTc6FGR7S/xGXSDzlc2lF53FkoBLhMq6ZRSoqzhLqagmR0JoLfycu48bZdYB0mXJLpEs6/UB/53mfp0xRMUfVvxHA7wB4VkR2tbf9e8wu+L8WkbsAvAbgt7KO6DjOopNc+CGEpzB3Lb5fXdjpOI7TD9xzz3EKyKJm2bVsHHaMYJvMdHDIcBi57ur3RO1Hlq+O2lbJaHZm2f8iZeo1bMwqZZwdPxIH6TQNG5Qz7kxNc/Ud/Z25Qs9Lz/9I9ZmaivuwXmLpDbd+9LbuYyyb8zwF5TC9VA9irAxPSsfoIYNxjeZiZYvuJavu+cKf+I5TQHzhO04B8YXvOAVkEarlvmXnWO+ELRu4k5z3sBbc58ob3x+1/+93j6sxXG3n2LHXo/bpaR3YM0PvgA+8/GrUHh8/kpwrJ+I4cOSo6vPf/t1nozbb8wBQp2QjgxQgxMFAAHDjL1/bdW7WueZrwn2sa8b6iGWbp95ppzIyW1jH4f1ywJBldy+ELW6ey8QYSyXIrZ7TiT/xHaeA+MJ3nALiC99xCogvfMcpIIvqwPPGiRNq2/oVK6I2O8TUe8igAmiB6Z99NPY23vH9p9SYyUl2vomDNf7nZ76ixlxyRRy48+bh2OmHA38AXbaKnYk+/8n/pMZwHxbyZvcbi3eDg8OqDzNMmYNTwh2ghTqV6afHbDQ5paeZVMBWTsnrhRDuLOcu0/mJSJ0rS/zrFCNzz7Q/8R2ngPjCd5wC4gvfcQpIf238EKKkDUMVHSTCgQycWMEKflD7yHAGGaRjr1u/SY05cvRnUbtei+3oV16Oy1ADwL59z0XtGbLF2Z4HgIGB2HLjoJ2BAW0vTk/HiUNEjKAQyqLLGsW6Dfo787mbqtVUH0aVvCZdxiyHztmTy/pW5IAtvhcqGYEwrAtYSTUGE/PNsflrlBiF7y/Atvvni+Xe1nn35Drz+BPfcQqIL3zHKSC+8B2ngPT9PX6z8ZadY1UoSVVU4SSTADBE7565KmzOfq665WrVZ8eOf4jabHtXjeQdbGtzoA8H4ABQiUSq1ZGonZPAgRNmWHNhW/WGD/+iGsPnrpZh7/L76UDnNieRhWWb8veuk31eyrCjc+xz1hv4Xb9VvYbvXUujSB3Huv/5OvL1MCv39qAd+BPfcQqIL3zHKSC+8B2ngPjCd5wC0ldxr9lo4tSxU2fbPzuks9FMnIwdU4ZGKWtMRU95oBz//zU8VFV9To3H+z1+KM64M3FSZ7BhJxl2gGHhzoLFGM6CY2KIhkyrxVliLKEoFqnYeWjVxjjTMABMzMROSpaTFTNN4hI7xFjiZMo5B9BimBL7jDHch9uWoBZIzONAMEss5nLtWZV0+DwZAUNCfaZJKOWgNSC+x9yBx3GcOfGF7zgFxBe+4xSQvtr4jXoDb77+xtn2s08+q/ooG4XblkMG9Zk8pW3kvc+9ELXZUeKK665RY276ld+M2lMTsQ6wffu31ZiZmcmozbpAtaqTYSiHHbJdq4YuMEA2veXAkxqz83s7VZ8XRmJ9pFJN2/hMTpBLylHFolQmx5q6DripDHafb06WXWZgwMiyW+p+vkMz/X1aLSPLbiX+js16s+vnzAlDq7LwJ77jFBBf+I5TQHzhO04BWdRkm8Gw69juGSBbim0eABCywQ6+8rrq89pru6N2id6hnj6lK+mMLhuL2m+//F1R+4/u+Jwac+yNY1H7i/f9h6g9M5N+Rz88tCweYyToZK3AqqSjg33ic2fZ1Wx78/keKBmVjBr0rpz8KvhzAAglCj6x3q+TPW7Z9KkxbL9b9nyL7HG+50xbnExtPk+9aAkAEBL3v0XnvZybJtSf+I5TQHzhO04BSS58ERkSkadF5Eci8ryI/Gl7+2Uisk1E9ojI10REV2B0HGdJkvPEnwFwawjhagDXALhNRG4A8FkAnw8hXA7gOIC7zt80HcdZSJLiXphVKc4oR5X2nwDgVgC/3d7+IIDPAPhiYl/zLunbqFE1FUPsYHFm/OSbqs/IyHI1l07ePHJAjTlB+zl+/FDU3vaDx9QYzrBzxyf+TdR+5Ot/ocZMTZ2K2r1kebWz98bnqkRBOyxwAlpAs4KiGBbzatOUcXY4/WMw577IOQ/NRvcMTjmw6DloODGxg44SBA0HHt7GorQ5Fz6O8axu4jxV0hGRkojsAnAYwGMAfgrgRAjhzF1yAMDFmcd0HGeRyVr4IYRmCOEaAJsAXA/gvVY3a6yI3C0iO0Rkh1U3znGc/jMvVT+EcALAEwBuADAmImd+B24CoF+ez465P4SwJYSwJSsW3XGc807SgBORdQDqIYQTIjIM4AOYFfYeB3AHgIcA3Ang4fkevDajM+YybEs1m4YDCTlPyIC2Xbly7OnTscMOJ7YAgBDiY09OjkdtDsix2P7U1qh9003/QvV56qlvxvulBCDVoVE1ZnQ0rirMGYABnYiDHZIsuzrpNGNcMtYBlL1rOPCkjgto7UD1sHQB1gF6qNTLzkQ59ymTpWUZcgRrFKrar6G5RMFLmd83x3NvI4AHRaSE2V8Ifx1CeFREXgDwkIj8ZwA7ATyQdUTHcRadHFX/xwCuNba/jFl733GcCwz33HOcAuIL33EKSP9LaHVkpLEi7dh5gp1QGhlCi5X9lktGVyrVrm1AO+NwNp3h4dgpyDrOxMTJqP3iT7apMTfeGGf6OULORPv3x5GFADA+fpTmqp1MRkdXRu2RkbidE6mWQy8RcTniVy9Rfwwf24oubDUpIg603wzHoaxMURmoiFUuT2ZECvZyzfyJ7zgFxBe+4xQQX/iOU0D6bOPHQTpWBhh2YGgGciAxbKfaVGzTN+ozqs/p0yeituWww7CnIdulnOEGACqV7iW7Laefl158OmpfetmVUftd73q/GrPvlThDcSvoczk4GGfpqVZjHYOz+QJAeaCHWyJh35olsBtcCcjIxEuOQDN0nXO0A87MG0KO3kAZjK3Etgkb3nJasvSFJDQ365p1bvNKOo7jzIkvfMcpIL7wHaeA9NXGDyFE77nZNj/Tp5McW5D7TFAwDaCTXaxYsTZqW0EuXC335Mm4uu+aNW9TY1auXBe1B4SSLxgBN5OT8dz27/9J1N648Z1qzHvee0PU/ulPd6k+TfpO1dF0dCS/E2aNwgoS4Sy0oRHrMjkJJyxSVZXyfAHmn5ijRddsoKWfj3wf8lysykacoCQng25WEpbB+S9jf+I7TgHxhe84BcQXvuMUEF/4jlNA+izuxc4GjZoW1FQppQxBJydIgctScfCMlRZsbGx91ObyV4cO7VNjUsE/luMQC0FcDmvfPl1OnJ2HRkZWqD6MKjNd0yWpSiQ4cTloq4wVZ+tlMc+6PjlZaflYWaXAab8c9GIJjapMWyILTg7K8Qz63raEx1Q5r5TY5w48juPMiS98xykgvvAdp4AsaplsK5EC5b5QGU5zKrtYwTOcVZeTdQwaiTjYqadMkxsQ/f8mOwqxjW8lCWFCiwOV9Hmq1WK9YfXqjaoPaxIVqgjD9rw5lwz9pNGDDWwFaDE5Nn0KZfMah+Xy4TkaBTPAY4yEGZzgw+rDKJvf0Cg688PkXgp/4jtOAfGF7zgFxBe+4xSQvifi6AzSsd7jN5v0HpZsnHpT28g5thLDlXWmpk+rPhzsw0Evq1ZfpMYsW7YqatcpKYj1npVrCvLcLFuXk2ta2sGyZXFyTT62VSGG3xvzsS0bE3T++d2/lVR1gBJnWokrQrl7Ek8zwUeze1CXdS6t5Bbd9mHtJ7TiPtY+rerEKXICnHrxM/AnvuMUEF/4jlNAfOE7TgHxhe84BaTvDjydQkTTcoygbexkYgl5OdlPtBhW79oGgHI5HlOuxllrV6/STjMNqqTDQSJBDKclcgzi8tzlknYuSmXzBXSWoRxU4AjdIVaWWiWgIe10woKgBQt+qYo91rECFdduZghhOVlvmKx7kAXNHhx47Gd1x348SMdxnLnwhe84BcQXvuMUkEXIsqsTFHTC9hXrADnVU6wgHbbXG434q3PVmdkdcxWcOOilVNYVatnGV32Mr1+iLKmcsIE1AAAYGGBdQNt2w8vj75Rju3KyDiZnH3zNzGCgAXasyXEMiudmOQax85CqrGM40aQceHIchXiu5UF9b8xMz78SUPK4tB9PxOE4zpz4wnecApK98EWkJCI7ReTRdvsyEdkmIntE5GsiMpjah+M4S4P52Pj3ANgN4ExWx88C+HwI4SER+R8A7gLwxW47EJHI1u6pAojxfrRcie22ycnlqk+V3sGrqrVGsguhMqnlSjp4hpNpsqbBiTmsMWzTl4wKtpbdz3DSkkFKxGEmiyBbu0Hv9c1EEBk6DGMF5aTmwgFbHOgDaLtfJ+IwbODEcfhzEz5vRgAawz4GgL4m6nz3WJWIyXrii8gmAB8G8KV2WwDcCuAb7S4PAvjIgszIcZzzTu5P/S8A+GO8lbhoDYATIZzNIXwAwMXWQBG5W0R2iMgODlF1HGdxSC58Efl1AIdDCD/s3Gx0Nd8jhBDuDyFsCSFssX7mOo7Tf3KM7BsB/IaIfAjAEGZt/C8AGBORcvupvwnA6+dvmo7jLCTJhR9C+DSATwOAiNwC4I9CCJ8Qka8DuAPAQwDuBPBw+nASBcvkZMxlLAcTFnCWLRtTfY4d614iumWIe4MUHMNOQOxEM7stnh+X2uYAHEAHECmnE8NRiIXFVsuocEPiF2fcsc5ls9k92MQSBNOZcXoTpFKiofVzVWe77eHAOQIan4deRDerqg8fhsU+Yzehh5fy5/Ie/1MA/q2I7MWszf/AOezLcZw+Mq9HbgjhCQBPtP/9MoDrF35KjuOcb9xzz3EKSN+z7HZWialb1XLJjuOgCqvCKNt1w8u0PT80NEr7Tdvr7PTDtmuzaWSpJZu4SsE/tYxXmuycYwU2cawG6w8WrKlYAR05yS6YlF1tJVxh5y2rYg/vJ8uZZSEcYHISZHCfHjI9W2P4/HPb0iy4QlIO/sR3nALiC99xCogvfMcpIH1Pttn5vtwK1ODAi5xqpw0O3jASP6wa2xC1jx9/I2rX63E1G8BKyBknUlCBPtABNU16v25Xy+XEmbEBP0O+AIB+t2/5LnBCD6VRWHqJSm5KiT/Nd/+pBKlqSFYgjLqK3CfDrlZzs551PbyD70ULyRmjEtHQNeLkrTzGq+U6jjMnvvAdp4D4wnecAuIL33EKSN+z7HYKV2Y2F8qgwgJgqaLFpZxSzkOjceZdFkmGh3XWHhbvRkfistNc3hqwquLEasuIcRwrQCj1eZlEoDESL3Mwg3RITOI+nJEHSAtbZgZdPq6RMVdl522xM0s6g1BOJaZUYIzl6GRmDk6QEzCkKvLQccS4Zp3OULlFgPyJ7zgFxBe+4xQQX/iOU0D6auOLCETe+r+mVtPOLCPL4mCamak4qKU+Y1S1pYAPy4FHHWc0ttenpk6pPpxUg51mqqV0JZ0y9bHs9QFhjaJ7NRtA24JWkE6qKo5FKpDHcuBhkpliAeU0I635Z7LmDRlpAAAF5ElEQVS19ptKCpKjN2jHoYznY4YzUS8BQ6x9WNpCrHHlGfn+xHecAuIL33EKiC98xykgvvAdp4D0OTpPUC6/FYn2S7ffqHrs/N7OqN2oW9FstFcSdMplq+RULEqtX39J1N679xk1hjPfdM59LljMyykiwhF8aHJGm+5lnAHbsalRp/JdRunm+WI5wDBWaauexnCZ7GQ2X4OMbLgqyxPS55tJlecGeps/i3lW+e34GMldAvAnvuMUEl/4jlNAfOE7TgHpq41fqVSwdu3Gs+1dW3epPju2fTdqT0yOR23OfAsA1113W9RuNHRW2upQbJ8vXxlnrKlW4yAeAJiaOh21ORNOZVDXAmS7LcfGF+luH+ZoCxOnxtW2FWtWGD3fwrKrc4KiUihb1sigq8YYjilcIUZVGCrpuXHGIN6vNYZLWrMd3eSUxsZccspxc1nsklFJio/F87fu7V60G3/iO04B8YXvOAXEF77jFJC+2viTk+PYufP7Z9tW9thm4p31yZNH1LYXXvh/UfvKq29SfTjjbKUat9esudicbzfYngSAWm0qanNiDmsMJxLJydjKmBl/KaAm5/062/Q8N4vkfA272rKbGfbHEHoHHww7mm14XXnY0BICVy6K59bLOeC5AkCLtA7rXuD5cx9T1+jYr2fZdRxnTnzhO04B8YXvOAXEF77jFJC+inutVisS9N44tE/1sUtMdWcZZdMpW5l4yRGCBZsVK9aoMaO0314ca5pUhqs1YIl78Xxzgjk4405OOaZUCXIgT0BjLMGsEyuwZ5Dmn5MxV3fIEEH5vBjnUt0bnJk3wwEph5zzz6gM0lYGno5tlqho7jerl+M4/6Twhe84BcQXvuMUEMlKZrBQBxN5E8CrANYC0J44S5MLaa7AhTXfC2muwIUx37eHENalOvV14Z89qMiOEMKWvh+4By6kuQIX1nwvpLkCF958u+E/9R2ngPjCd5wCslgL//5FOm4vXEhzBS6s+V5IcwUuvPnOyaLY+I7jLC7+U99xCkhfF76I3CYiL4rIXhG5t5/HzkFEviwih0XkuY5tq0XkMRHZ0/571WLO8QwisllEHheR3SLyvIjc096+VOc7JCJPi8iP2vP90/b2y0RkW3u+XxORtB90nxCRkojsFJFH2+0lO9f50reFLyIlAP8dwK8BuALAx0Xkin4dP5OvALiNtt0LYGsI4XIAW9vtpUADwB+GEN4L4AYA/6p9PpfqfGcA3BpCuBrANQBuE5EbAHwWwOfb8z0O4K5FnCNzD4DdHe2lPNd50c8n/vUA9oYQXg4h1AA8BOD2Ph4/SQjhSQDHaPPtAB5s//tBAB/p66TmIIRwMITwTPvfpzB7g16MpTvfEEI4k7a40v4TANwK4Bvt7UtmviKyCcCHAXyp3RYs0bn2Qj8X/sUA9ne0D7S3LXU2hBAOArOLDcD6RZ6PQkQuBXAtgG1YwvNt/3TeBeAwgMcA/BTAiRDCmZzRS+me+AKAPwZwJjRvDZbuXOdNPxe+FS/orxTOERFZBuCbAD4ZQuieJHCRCSE0QwjXANiE2V+A77W69XdWGhH5dQCHQwg/7NxsdF30ufZKP+PxDwDY3NHeBOD1Ph6/Vw6JyMYQwkER2YjZp9WSQEQqmF30fxVC+Jv25iU73zOEEE6IyBOY1SbGRKTcfpIulXviRgC/ISIfAjAEYAVmfwEsxbn2RD+f+NsBXN5WRgcBfAzAI308fq88AuDO9r/vBPDwIs7lLG2b8wEAu0MIf97x0VKd7zoRGWv/exjABzCrSzwO4I52tyUx3xDCp0MIm0IIl2L2Pv1+COETWIJz7ZkQQt/+APgQgJcwa9v9ST+PnTm/rwI4CKCO2V8od2HWttsKYE/779WLPc/2XG/C7E/NHwPY1f7zoSU836sA7GzP9zkA/7G9/R0AngawF8DXAVQXe64071sAPHohzHU+f9xzz3EKiHvuOU4B8YXvOAXEF77jFBBf+I5TQHzhO04B8YXvOAXEF77jFBBf+I5TQP4/gt5ZLqzC4S4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_d[3000].squeeze(),cmap='bone')\n",
    "print(y_train[3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/envs/mlenv/lib/python3.7/site-packages/skimage/io/_plugins/matplotlib_plugin.py:80: UserWarning:\n",
      "\n",
      "Float image out of standard range; displaying image with stretched contrast.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmps = datagen.flow(X_train_d, y_train_ohe, batch_size=1)\n",
    "\n",
    "for tmp in tmps:\n",
    "    imshow(tmp[0].squeeze(),cmap='bone')\n",
    "   \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Comparing CNNs with Different Parameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/andrew/anaconda3/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/andrew/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# what if we just want to use the validation data??\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "l2_lambda = 0.0001\n",
    "\n",
    "# Use Kaiming He to regularize ReLU layers: https://arxiv.org/pdf/1502.01852.pdf\n",
    "# Use Glorot/Bengio for linear/sigmoid/softmax: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf \n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu',\n",
    "               data_format=\"channels_last\")) # more compact syntax\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu',data_format=\"channels_last\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn.add(Conv2D(filters=64,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu',data_format=\"channels_last\")) # more compact syntax\n",
    "\n",
    "cnn.add(Conv2D(filters=64,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn.add(Conv2D(filters=128,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu',data_format=\"channels_last\")) # more compact syntax\n",
    "\n",
    "cnn.add(Conv2D(filters=128,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu',data_format=\"channels_last\"))\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dropout(0.25)) # add some dropout for regularization after conv layers\n",
    "cnn.add(Dense(128, \n",
    "              activation='relu',\n",
    "              kernel_initializer='he_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "       ))\n",
    "cnn.add(Dropout(0.5)) # add some dropout for regularization, again!\n",
    "cnn.add(Dense(NUM_CLASSES, \n",
    "              activation='softmax', \n",
    "              kernel_initializer='glorot_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "             ))\n",
    "\n",
    "# Let's train the model \n",
    "cnn.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "              optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "              metrics=['acc', f1_m])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/andrew/anaconda3/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      " 893/2174 [===========>..................] - ETA: 4:19 - loss: 6.5173 - acc: 0.0482 - f1_m: 0.0128"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a1ece7f56860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                  )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history =cnn.fit_generator(datagen.flow(X_train_d, y_train_ohe, batch_size=32), \n",
    "                  steps_per_epoch=int(len(X_train)/32), # how many generators to go through per epoch\n",
    "                  epochs=50, verbose=1,\n",
    "                  validation_data=(X_test_d,y_test_ohe),\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=2)]\n",
    "                 )\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_net(cnn, X_test_d, y_test, title_text='Using Expansion:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import average, concatenate\n",
    "from keras.models import Input, Model\n",
    "\n",
    "num_ensembles = 3\n",
    "l2_lambda = 0.000001\n",
    "\n",
    "input_holder = Input(shape=(img_wh, img_wh, 1))\n",
    "\n",
    "# start with a conv layer\n",
    "x = Conv2D(filters=32,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', data_format=\"channels_last\")(input_holder)\n",
    "\n",
    "x = Conv2D(filters=32,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu')(x)\n",
    "input_conv = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "branches = []\n",
    "for _ in range(num_ensembles):\n",
    "    \n",
    "    # start using NiN (MLPConv)\n",
    "    x = Conv2D(filters=32,\n",
    "                   input_shape = (img_wh,img_wh,1),\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='linear', data_format=\"channels_last\")(input_conv)\n",
    "\n",
    "    x = Conv2D(filters=32,\n",
    "                   kernel_size=(1,1),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "    \n",
    "    x = Conv2D(filters=64,\n",
    "                   input_shape = (img_wh,img_wh,1),\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='linear', data_format=\"channels_last\")(x)\n",
    "\n",
    "    x = Conv2D(filters=64,\n",
    "                   kernel_size=(1,1),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='linear', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "    # add one layer on flattened output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.50)(x) # add some dropout for regularization after conv layers\n",
    "    x = Dense(64, \n",
    "              activation='relu',\n",
    "              kernel_initializer='he_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "            )(x)\n",
    "    \n",
    "    x = Dense(NUM_CLASSES, \n",
    "              activation='relu',\n",
    "              kernel_initializer='he_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    \n",
    "    # now add this branch onto the master list\n",
    "    branches.append(x)\n",
    "\n",
    "# that's it, we just need to average the results\n",
    "x = concatenate(branches)\n",
    "\n",
    "x = Dense(NUM_CLASSES, \n",
    "          activation='softmax', \n",
    "          kernel_initializer='glorot_uniform',\n",
    "          kernel_regularizer=l2(l2_lambda)\n",
    "         )(x)\n",
    "\n",
    "# here is the secret sauce for setting the network using the \n",
    "#   Functional API:\n",
    "cnn_ens = Model(inputs=input_holder,outputs=x)\n",
    "\n",
    "cnn_ens.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_ens.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['acc', f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history2 =cnn_ens.fit_generator(datagen.flow(X_train_d, y_train_ohe, batch_size=128), \n",
    "                  steps_per_epoch=int(len(X_train)/128), # how many generators to go through per epoch\n",
    "                  epochs=50, verbose=1,\n",
    "                  validation_data=(X_test_d,y_test_ohe),\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=2)]\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summarize_net(cnn_ens, X_test_d, y_test, title_text='Using Expansion:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception style architecture\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Add\n",
    "from keras.layers import average, concatenate\n",
    "from keras.models import Input, Model\n",
    "\n",
    "l2_lambda = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "input_holder = Input(shape=(img_wh, img_wh, 1))\n",
    "\n",
    "# start with a conv layer\n",
    "x = Conv2D(filters=32,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               data_format=\"channels_last\")(input_holder)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Conv2D(filters=64,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               data_format=\"channels_last\")(x)\n",
    "\n",
    "\n",
    "x_split = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = SeparableConv2D(filters=64,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               depth_multiplier = 1, # controls output channels\n",
    "               data_format=\"channels_last\")(x_split)\n",
    "\n",
    "\n",
    "x_split = Add()([x, x_split])\n",
    "x_split = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x_split)\n",
    "\n",
    "\n",
    "x = SeparableConv2D(filters=64,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               depth_multiplier = 1, # controls output channels\n",
    "               data_format=\"channels_last\")(x_split)\n",
    "\n",
    "\n",
    "x_split = Add()([x, x_split])\n",
    "x_split = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x_split)\n",
    "x_split = SeparableConv2D(filters=128,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               depth_multiplier = 1, # controls output channels\n",
    "               data_format=\"channels_last\")(x_split)\n",
    "x = Activation(\"relu\")(x_split)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(NUM_CLASSES,activation=\"softmax\")(x)\n",
    "\n",
    "xception = Model(inputs=input_holder,outputs=x)\n",
    "\n",
    "xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed up by training by not using augmentation, perhaps there are faster ways??\n",
    "xception.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['acc', f1_m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history3 =xception.fit_generator(datagen.flow(X_train_d, y_train_ohe, batch_size=32), \n",
    "                  steps_per_epoch=int(len(X_train)/32), # how many generators to go through per epoch\n",
    "                  epochs=50, verbose=1,\n",
    "                  validation_data=(X_test_d,y_test_ohe),\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_net(xception, X_test_d, y_test, title_text='Using Expansion:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Visualize Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,11))\n",
    "plt.subplot(2,3,1)\n",
    "plt.ylabel('5 layers CNN acc and val_acc')\n",
    "plt.xlabel('epochs CNN')\n",
    "plt.plot(history.history['f1_m'])\n",
    "\n",
    "plt.plot(history.history['val_f1_m'])\n",
    "\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('5 layers CNN acc and val_acc')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epochs CNN')\n",
    "\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.ylabel('Ensemble Nets acc and val_acc')\n",
    "plt.xlabel('epochs Ensemble')\n",
    "plt.plot(history2.history['f1_m'])\n",
    "\n",
    "plt.plot(history2.history['val_f1_m'])\n",
    "\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.ylabel('Ensemble Nets Loss and val_loss')\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.xlabel('epochs Ensemble')\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.ylabel('Xception Nets acc and val_acc')\n",
    "plt.xlabel('epochs Xception')\n",
    "plt.plot(history3.history['f1_m'])\n",
    "\n",
    "plt.plot(history3.history['val_f1_m'])\n",
    "\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.ylabel('Xception Nets Loss and val_loss')\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.xlabel('epochs Xception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the figures above the following, we found out that network in network Ensemble is the best in terms of loss and validation accuract, \n",
    "\n",
    "Xception did the worst in this set even though we have more than 77 thousand parameters to optimize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Comparing CNN Performance with MLP\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def compare_mlp_cnn(cnn, mlp, X_test, y_test):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    if cnn is not None:\n",
    "        X_test_d = np.expand_dims(X_test.reshape((-1,img_wh,img_wh)), axis=3)\n",
    "        yhat_cnn = np.argmax(cnn.predict(X_test_d), axis=1)\n",
    "        acc_cnn = mt.accuracy_score(y_test,yhat_cnn)\n",
    "        plt.subplot(1,2,1)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_cnn)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f')\n",
    "        plt.title('CNN: '+str(acc_cnn))\n",
    "    \n",
    "    if mlp is not None:\n",
    "        yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "        acc_mlp = mt.accuracy_score(y_test,yhat_mlp)\n",
    "        plt.subplot(1,2,2)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_mlp)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2f')\n",
    "        plt.title('MLP: '+str(acc_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# make a 3 layer keras MLP\n",
    "mlp = Sequential()\n",
    "mlp.add( Dense(input_dim=X_train.shape[1], units=30, activation='relu') )\n",
    "mlp.add( Dense(units=15, activation='relu') )\n",
    "mlp.add( Dense(NUM_CLASSES) )\n",
    "mlp.add( Activation('softmax') )\n",
    "\n",
    "mlp.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.flatten().shape)\n",
    "\n",
    "compare_mlp_cnn(cnn,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Exceptional Work: Transfer Learning**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69599, 50, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning:\n",
      "\n",
      "`imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'arr' does not have a suitable array shape for any mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e4b2d3468055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# upsample images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mx_train_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mx_train_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_up\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e4b2d3468055>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# upsample images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mx_train_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mx_train_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_up\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimresize\u001b[0;34m(arr, size, interp, mode)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \"\"\"\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignedinteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.7/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mtoimage\u001b[0;34m(arr, high, low, cmin, cmax, pal, mode, channel_axis)\u001b[0m\n\u001b[1;32m    325\u001b[0m                                 ((3 in shape) or (4 in shape)))\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         raise ValueError(\"'arr' does not have a suitable array shape for \"\n\u001b[0m\u001b[1;32m    328\u001b[0m                          \"any mode.\")\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'arr' does not have a suitable array shape for any mode."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model, Input\n",
    "from keras.applications.vgg19 import VGG19, decode_predictions, preprocess_input\n",
    "from keras.layers import SeparableConv2D, Add, Flatten, Dense, average, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "# copy train and test sets\n",
    "x_train = X_train_d.copy()\n",
    "x_test = X_test_d.copy()\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# upsample images\n",
    "x_train_up = [imresize(x, size=(64,64,3), interp='nearest') for x in x_train]\n",
    "x_train_up = np.stack(x_train_up, axis=0)\n",
    "print(x_train_up.shape)\n",
    "\n",
    "x_test_up = [imresize(x, size=(64,64,3), interp='nearest') for x in x_test]\n",
    "x_test_up = np.stack(x_test_up, axis=0)\n",
    "print(x_test_up.shape) \n",
    "\n",
    "model = VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "x = x_train_up[0]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "%time \n",
    "preds = model.predict(x)\n",
    "preds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_up = preprocess_input(x_train_up)\n",
    "x_test_up = preprocess_input(x_test_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_train_vgg = model.predict(x_train_up)\n",
    "x_test_vgg = model.predict(x_test_up)\n",
    "print(x_train_vgg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = Input(shape=x_train_vgg[0].shape)\n",
    "x = Flatten()(input_x)\n",
    "x = Dense(NUM_CLASSES, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model2 = Model(inputs=input_x, outputs=predictions)\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ohe_vgg = y_train_ohe[:x_train_vgg.shape[0]]\n",
    "\n",
    "model2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(x_train_vgg, y_train_ohe_vgg, epochs=5, batch_size=64, verbos=1, validation_data=(x_test_vgg, y_test_ohe[x_test_vgg.shape[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_net(model2, x_test_vgg, y_test[:x_test_vgg.shape[0]], title_text='Transfer Learning, VGG19:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. References**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
